Data Preprocessing and Forecast
===============================

The following section explains the most important stations of the forecast. In particular, the most relevant functions and their usage will be explained.

Data Source: https://opendata.cityofnewyork.us/

.. note:: The Following Part only highlights the most important Functions.

   For detailed Information and examples look at the original Github:

   https://github.com/Hakan1998/NYC-Taxi-Demand-Forecast

Data Preprocessing
==================

Spatial Analysis
----------------

After a basic data cleaning, we want to know in which region how many pickups took place. Our raw data only contains the information about which pickup had which pickup coordinates.

.. _process_taxi_data:

.. function:: process_taxi_data(data_train, shapefile_path)

   This function processes taxi data by performing spatial operations with zone shapefiles.

   :param data_train: A pandas DataFrame containing taxi data.
   :type data_train: pandas.DataFrame
   :param shapefile_path: The file path to the shapefile containing zone information.
   :type shapefile_path: str
   :return: A pandas DataFrame with processed taxi data including zone and borough information.
   :rtype: pandas.DataFrame

   This function loads the zone shapefile specified by ``shapefile_path``, transforms it to the EPSG:4326 coordinate system for consistent comparison, and performs spatial operations with the taxi data provided in the DataFrame ``data_train``. It extracts relevant columns such as "Trip_Pickup_DateTime", "pickup_day", "pickup_hour", "Start_Lon", "Start_Lat", "geometry", "zone", and "borough". The resulting DataFrame includes these columns along with zone and borough information merged from the shapefile. The function returns this processed DataFrame.

   Example Usage::

      import geopandas as gpd
      import pandas as pd

      def process_taxi_data(data_train, shapefile_path):
          gdf_zones = gpd.read_file(shapefile_path).to_crs('EPSG:4326')
          gdf_taxi = gpd.GeoDataFrame(data_train, geometry=gpd.points_from_xy(data_train['Start_Lon'], data_train['Start_Lat']))
          gdf_taxi.crs = "EPSG:4326"
          taxi_with_zones = gpd.sjoin(gdf_taxi, gdf_zones, how='left', op='within')
          result_df = pd.merge(taxi_with_zones[["Trip_Pickup_DateTime", "pickup_day", "pickup_hour", "Start_Lon", "Start_Lat", "geometry", "zone", "borough"]].rename(columns={'geometry': 'geo_point'}),
                                   gdf_zones[['zone', 'borough', 'geometry']], on=['zone', 'borough'], how='left')
          return result_df

      # Example usage
      data = ...  # Load taxi data
      shapefile_path = "/path/to/shapefile.shp"
      processed_data = process_taxi_data(data, shapefile_path)

Data Visualization
------------------

After the data has been processed, a visual representation can aid in better understanding the data. To achieve this, we will sample 1000 pickup trips using the following function:

.. _plot_taxi_trips:

.. function:: plot_taxi_trips(data, shapefile_path, sample_size)

.. code-block:: python

   import geopandas as gpd
   import matplotlib.pyplot as plt
   import pandas as pd

   def plot_taxi_trips(data, shapefile_path, sample_size=10000):
       nyc_map = gpd.read_file(shapefile_path).to_crs(epsg=4326)
       
       gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data['Start_Lon'], data['Start_Lat']), crs=4326)
       gdf_sample = gdf.sample(sample_size, random_state=42)
       
       fig, ax = plt.subplots(figsize=(10, 10))
       nyc_map.plot(ax=ax, color='lightgrey', linewidth=0.5, edgecolor='k')
       ax.scatter(gdf_sample['geometry'].x, gdf_sample['geometry'].y, marker='o', color='red', s=5)
       ax.set_xlabel('Longitude')
       ax.set_ylabel('Latitude')
       ax.set_title(f'Taxi Trips Sample (Size: {sample_size}) in New York City')
       plt.show()

       borough_percentage = (data['borough'].value_counts() / len(data)) * 100
       borough_counts = data['borough'].value_counts().reset_index()
       borough_counts.columns = ['borough', 'data_points_count']
       borough_counts = pd.merge(borough_counts, pd.DataFrame({'borough': borough_percentage.index, 'percentage': borough_percentage.values}), on='borough')
       
       return borough_counts

This function ``plot_taxi_trips`` generates a plot displaying a sample of taxi trips on a map of New York City. It takes as input the taxi data, the file path to the shapefile containing zone information, and an optional parameter ``sample_size`` which defaults to 10000. The function samples ``sample_size`` number of trips randomly, plots them on the map, and provides a breakdown of trip counts by borough.

Here's an example plot generated by the function:

.. image:: Bild Samples NYC.png
   :width: 800px
   :height: 800px
   :alt: Taxi Trips Sample in New York City

Time Binning
------------

.. function:: one_hour_time_binning(data_frame)

   Bin the taxi trip data into one-hour intervals and calculate the demand for each zone in each hour.

   :param data_frame: A pandas DataFrame containing taxi trip data.
   :type data_frame: pandas.DataFrame
   :return: A DataFrame with the demand for each zone in each one-hour interval.
   :rtype: pandas.DataFrame

   This function converts the 'Trip_Pickup_DateTime' column in the DataFrame to datetime format. It then defines time bins with one-hour intervals covering the entire time range of the data. Next, it creates a new column 'time_bin' based on these time bins. The function then groups the data by 'zone' and 'time_bin' and counts the number of trips in each group, representing the demand for each zone in each one-hour interval. Finally, it returns a DataFrame containing this demand data. If you want to change the time duration just change the freq factor in the time_bins variable.


   .. code-block:: python

         import pandas as pd

         def one_hour_time_binning(data_frame):
             # Convert 'Trip_Pickup_DateTime' to datetime
             data_frame['Trip_Pickup_DateTime'] = pd.to_datetime(data_frame['Trip_Pickup_DateTime'])

             # Define the time bins (1-hour intervals)
             time_bins = pd.date_range(start=data_frame['Trip_Pickup_DateTime'].min(), end=data_frame['Trip_Pickup_DateTime'].max(), freq='1H')

             # Create a new column 'time_bin' based on the time bins
             data_frame['time_bin'] = pd.cut(data_frame['Trip_Pickup_DateTime'], bins=time_bins, labels=time_bins[:-1])

             # Group by 'zone' and 'time_bin' and count the number of trips in each group
             processed_data = data_frame.groupby(['zone', 'time_bin']).size().reset_index(name='demand')

             return processed_data

         # Example usage
         data = ...  # Load processed taxi data
         demand_data = one_hour_time_binning(data)

         # Output the first few rows of the demand data
         print(demand_data.head())

EDA
===========

To sum up the EDA will test the factors Stationarity, Trend and Saisonality for each zone. Since the trend is mostly better seen visualy the function test for Stationarity and Seasonality. 

.. function:: analyze_stationarity_seasonality(data)

   Analyze the stationarity and seasonality of the demand data for each zone.

   :param data: A pandas DataFrame containing taxi demand data with a 'time_bin' column and a 'zone' column.
   :type data: pandas.DataFrame
   :return: A DataFrame summarizing the stationarity and seasonality analysis for each zone.
   :rtype: pandas.DataFrame

   This function analyzes the stationarity and seasonality of the demand data for each zone. It first sets the 'time_bin' column as the index of the DataFrame. For each unique zone in the 'zone' column, it performs the following analyses:
   
   - **ADF Test (Augmented Dickey-Fuller)**: Determines if the time series is stationary. A p-value less than 0.05 indicates stationarity.
   - **KPSS Test (Kwiatkowski-Phillips-Schmidt-Shin)**: Tests for stationarity. A p-value greater than 0.05 indicates stationarity.
   - **Seasonal Component Extraction**: Uses Seasonal-Trend decomposition using LOESS (STL) to extract the seasonal component. If the seasonal component is not entirely NaN, the series is considered seasonal.

   The results for each zone are compiled into a DataFrame with the following columns: 'Zone', 'ADF Statistic', 'P-value (ADF)', 'KPSS Statistic', 'P-value (KPSS)', 'Is Stationary', and 'Seasonality'.


      .. code-block:: python

                     import pandas as pd
                     from statsmodels.tsa.stattools import adfuller, kpss
                     from statsmodels.tsa.seasonal import STL
               
                     def analyze_stationarity_seasonality(data):
                         data.set_index('time_bin', inplace=True)
                         results = []
               
                         for zone in data['zone'].unique():
                             subset = data[data['zone'] == zone]['demand']
                             adf_stat, adf_p = adfuller(subset, autolag='AIC')[:2]
                             kpss_stat, kpss_p = kpss(subset, regression='c')[:2]
                             seasonal_component = STL(subset, seasonal=13).fit().seasonal
                             seasonality = 'Yes' if not seasonal_component.isna().all() else 'No'
                             is_stationary = 'Stationary' if adf_p &lt; 0.05 and kpss_p &gt; 0.05 else 'Non-Stationary'
               
                             results.append([zone, adf_stat, adf_p, kpss_stat, kpss_p, is_stationary, seasonality])
               
                         return pd.DataFrame(results, columns=['Zone', 'ADF Statistic', 'P-value (ADF)', 'KPSS Statistic', 'P-value (KPSS)', 'Is Stationary', 'Seasonality'])







